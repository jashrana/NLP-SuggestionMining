{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6saNLUFqHb"
      },
      "source": [
        "# Assignment 2 - CT5120\n",
        "\n",
        "### Instructions:\n",
        "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **November 25, 2022**.\n",
        "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
        "- The assignment is worth $50$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
        "\n",
        "| Task | Marks for write-up | Marks for code | Total Marks |\n",
        "| :--- | :----------------- | :------------- | :---------- |\n",
        "| 1    |                  5 |              5 |          10 |\n",
        "| 2    |                  - |             10 |          10 |\n",
        "| 3    |                  5 |              5 |          10 |\n",
        "| 4    |                  5 |              5 |          10 |\n",
        "| 5    |                  5 |              5 |          10 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCWSEtNeGMsN"
      },
      "source": [
        "---\n",
        "\n",
        "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
        "\n",
        "\n",
        "Download train.csv, test_seen.csv and test_unseen.csv from the [Github](https://github.com/sharduls007/Assignment_2_CT5120) or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
        "\n",
        "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
        "](https://aclanthology.org/S19-2151/).\n",
        "\n",
        "We will be using test_seen.csv for benchmarking our model, hence it has label. On the other hand, test_unseen is used for [Kaggle](https://www.kaggle.com/competitions/nlp2022ct5120suggestionmining/overview) competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShQ2lPxmPfA4",
        "outputId": "df651146-abe3-4d3b-8960-23eb1d2b977b"
      },
      "outputs": [],
      "source": [
        "# !curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
        "# !curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
        "# !curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5x0c38rCGk23"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "import string, nltk\n",
        "\n",
        "# Read the CSV file.\n",
        "train_df = pd.read_csv('train.csv', \n",
        "                 names=['id', 'text', 'label'], header=0)\n",
        "\n",
        "test_df = pd.read_csv('test.csv', \n",
        "                 names=['id', 'text', 'label'], header=0)\n",
        "\n",
        "# Store the data as a list of tuples where the first item is the text\n",
        "# and the second item is the label.\n",
        "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
        "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
        "\n",
        "# Check that training set and test set are of the right size.\n",
        "assert len(test_texts) == len(test_labels) == 1360\n",
        "assert len(train_texts) == len(train_labels) == 5440"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Scj45oSpdQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 1: Data Pre-processing (10 Marks)\n",
        "\n",
        "Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pd8ed8NdlB_"
      },
      "source": [
        "\n",
        "\n",
        "Edit this cell to write your answer below the line in no more than 300 words.\n",
        "\n",
        "---\n",
        "* **Lowercase:** Lowercase is better to use in some cases where some algorithms are designed to take lowercase letters/words. We use lower() on each sentence to convert all the words into lowercase.\n",
        "\n",
        "* **Punctuation Removal:** Punctuation Removal is the process of removing punctuation marks (like \"!\",\",\",\"?\"). The idea behind this is that we do not require punctuations to carry out some of the tasks in NLP, but it can be quite nice if you use them for Sentiment Analysis. In our case, suggestions wouldn't usually contain any punctuation and will have little impact if we remove them from the dataset. We use string.punctuation library which is inbuilt in Python to check for any type of punctuation marks and if we find it, we just discard it.\n",
        "\n",
        "* **Tokeniation:** Tokenization is the process of splitting either a query, paragraph or a document into smallest unit i.e., a word. For e.g., the sentence \"I am a human\" can be tokenized as \"I\", \"am\", \"a\", \"human\" and this is a good practice of NLP structures as discrete elements can be processed by the NLP model and token occurances can be used as vector representing the document. The NLTK.tokenize package has a 'word_tokenize' method which automatically converts your text into a list of tokens.\n",
        "\n",
        "* **Stopword Removal:** Stopwords are the words which have the highest frequency in a document e.g., I, You, The, An, etc. So they provide almost no information or the meaning in any sentence. These sentences are better off removed from the sentence and hence this is called Stopword Removal. In NLTK library, we have stopwords function where there is a dictionary of stopwords defined, which we can use to filter out stopwords in a given document.\n",
        "\n",
        "* **Lemmatization:** Lemmatization is a preprocessing method where you bring a word to its base form, e.g., running to run, better to good. Lemmatization is still in its very early stages as not every word will be converted as there are many grammatical constraints, but its a good alternative to Stemming where you just cut the words and sometimes get non-sensical words. To perform lemmatization, import WordNetLemmatizer() class from nltk,stem package. To make sure Lemmatization works well, we have to define Parts-of=speech tagging to lemmatize words better, so we used wordnet from nltk.corpus package, and tag the words into either Adjective (ADJ), Nouns (NOUN), Verbs (VERB), or Adverbs (ADV) and then send both the word and the POS of the word to lemmatize them.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-xXQggaVKh"
      },
      "source": [
        "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "stopword = stopwords.words('english')\n",
        "lemma = WordNetLemmatizer()\n",
        "# Ref: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "# This function tags the word with appropriate POS\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jb7i3Le4aSYM"
      },
      "outputs": [],
      "source": [
        "# your code goes here\n",
        "def preprocessing(text_list):\n",
        "    new_list = []\n",
        "    for i in range(len(text_list)):\n",
        "        text_list[i] = text_list[i].lower()\n",
        "        text_list[i] = \"\".join([word for word in text_list[i] if word not in string.punctuation])\n",
        "        text_list[i] = word_tokenize(text_list[i])\n",
        "        text_list[i] = [word for word in text_list[i] if word not in stopword]\n",
        "        text_list[i] = [lemma.lemmatize(word, get_wordnet_pos(word)) for word in text_list[i]]\n",
        "        text_list[i] = \" \".join(text_list[i])\n",
        "    return text_list\n",
        "\n",
        "train_texts = preprocessing(train_texts)\n",
        "test_texts = preprocessing(test_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IUJunnfXItQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 2: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
        "\n",
        "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
        "\n",
        "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
        "\n",
        "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3gDsfB8xTGMg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5294117647058824"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Calculate tf-idf scores for the words in the training set.\n",
        "# ... your code goes here\n",
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
        "tfidf = TfidfTransformer()\n",
        "v_train = vectorizer.fit_transform(train_texts)\n",
        "tf_train = tfidf.fit_transform(v_train)\n",
        "  \n",
        "\n",
        "v_test = vectorizer.transform(test_texts)\n",
        "tf_test = tfidf.transform(v_test) \n",
        "\n",
        "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
        "# ... your code goes here\n",
        "NB_classifier = GaussianNB()\n",
        "NB_classifier.fit(tf_train.toarray(), train_labels)\n",
        "\n",
        "# Predict on the test set.\n",
        "predictions = []    # save your predictions on the test set into this list\n",
        "\n",
        "# ... your code goes here\n",
        "p = NB_classifier.predict(tf_test.toarray())\n",
        "predictions = p\n",
        "\n",
        "#################### DO NOT EDIT BELOW THIS LINE #################\n",
        "\n",
        "\n",
        "#################### DO NOT EDIT BELOW THIS LINE #################\n",
        "\n",
        "def accuracy(labels, predictions):\n",
        "  '''\n",
        "  Calculate the accuracy score for a given set of predictions and labels.\n",
        "  \n",
        "  Args:\n",
        "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
        "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
        "\n",
        "  Returns:\n",
        "    float: A floating point value to score the predictions against the labels.\n",
        "  '''\n",
        "\n",
        "  assert len(labels) == len(predictions)\n",
        "  \n",
        "  correct = 0\n",
        "  for label, prediction in zip(labels, predictions):\n",
        "    if label == prediction:\n",
        "      correct += 1 \n",
        "  \n",
        "  score = correct / len(labels)\n",
        "  return score\n",
        "\n",
        "# Calculate accuracy score for the classifier using tf-idf features.\n",
        "accuracy(test_labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDx_M2aTIncl"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 3: Evaluation Metrics (10 marks)\n",
        "\n",
        "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8jDzSU86xI1"
      },
      "source": [
        "Edit this cell to write your answer below the line in no more than 150 words.\n",
        "\n",
        "---\n",
        "Accuracy is not the best measure when the dataset is imbalanced, so in our case, too, it's not the best evaluation metric. Instead, we turn towards another metric to find out the reality of our model.\n",
        "\n",
        "For any classification problems, a *confusion matrix* for each class to evaluate on the terms of **precision**, **recall** and **f1-score** is beneficial for finding out how the model works and its effectiveness.\n",
        "\n",
        "* **Confusion Matrix:** It is a special type of error table which allows visualization of the performance of an algorithm, and is typically used in the Supervised Learning methods. It is a 2x2 matrix which shows numbers based on the values as the representation below in the table. Some of the concepts followed in the matrix are:\n",
        "    1. True Positive(TP): The values which are predicted as positive and match when evaluated it with the original labels, are called True Positives.\n",
        "    2. False Negative(FN): The values which are predicted as negative but the original labels are positive, are called False Negatives.\n",
        "    3. False Positives(FP): The values which are predicted as positive, but the original labels state them negative, are called False Positives.\n",
        "    4. True Negative(TN): The values which we predicted as negative and they matched the original labels, are called as True Negatives.\n",
        "\n",
        "|    Total Population (P+N)   |     Positive(PP)     |    Negative(PN)      |\n",
        "| :-------------------------- | -------------------: | -------------------: |\n",
        "|       Positive (P)          |  True positive (TP)  |  False negative (FN) |\n",
        "|       Negative (N)          |  False positive (FP) |  True negative (TN)  |\n",
        "\n",
        "*Source: [Wikipedia -> Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)*\n",
        "\n",
        "\n",
        "* Precision: Precision is a count of how many are actually positive values with respect to the values which were predicted as positive. The equation can be drawn as:\n",
        "$$ Precision = \\frac {TP}{TP + FP} $$ \n",
        "\n",
        "* Recall: Recall is a count of how many were predicted correctly with respect to all the positive classes. The equation can be drawn as:\n",
        "$$ Recall = \\frac {TP}{TP + FN} $$ \n",
        "\n",
        "* F1-Score: It is difficult to compare models with low precision and high recall or vice versa. To make them comparable, we use the F-measure or the F1-score. It is the harmonic mean which uses both precision and recall to find out the comparable way to include both precision and recall. The equation can be drawn as:\n",
        "$$ F1-score = \\frac {2 * Precision * Recall }{Precision + Recall} $$ \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ozD4SyyRDL3"
      },
      "source": [
        "\n",
        "\n",
        "In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UkUX5K0oMhKI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        precision    recall  f1-score\n",
            "class 0  0.486855  0.815661  0.609756\n",
            "class 1  0.660661  0.294511  0.407407\n"
          ]
        }
      ],
      "source": [
        "def evaluate(labels, predictions):\n",
        "  '''\n",
        "  Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
        "  \n",
        "  Args:\n",
        "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
        "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
        "\n",
        "  Returns:\n",
        "    float: A floating point value to score the predictions against the labels.\n",
        "  '''\n",
        "\n",
        "  # check that labels and predictions are of same length\n",
        "  assert len(labels) == len(predictions)\n",
        "\n",
        "  # score = 0.0\n",
        "  \n",
        "  #################### EDIT BELOW THIS LINE #########################\n",
        "\n",
        "  # your code goes here\n",
        "  classification_report = pd.DataFrame(index = ['class 0', 'class 1'], columns=['precision','recall','f1-score'])\n",
        "\n",
        "  matrix0 =  np.zeros((2,2))\n",
        "  matrix1 = np.zeros((2,2))\n",
        "\n",
        "  for i in range(len(predictions)):\n",
        "    if predictions[i] == 0 and labels[i] == 0:\n",
        "      matrix0[0,0] += 1\n",
        "    if predictions[i] == 1 and labels[i] == 0:\n",
        "      matrix0[0,1] += 1\n",
        "    if predictions[i] == 0 and labels[i] == 1:\n",
        "      matrix0[1,0] += 1\n",
        "    if predictions[i] == 1 and labels[i] == 1:\n",
        "      matrix0[1,1] += 1\n",
        "\n",
        "  precision = matrix0[0,0] / (matrix0[0,0] + matrix0[0,1])\n",
        "  recall = matrix0[0,0] / (matrix0[0,0] + matrix0[1,0])\n",
        "  f1= 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  list_row = [precision, recall, f1]\n",
        "  classification_report.loc['class 0'] = list_row\n",
        "\n",
        "\n",
        "  for i in range(len(predictions)):\n",
        "      if predictions[i] == 1 and labels[i] == 1:\n",
        "        matrix1[0,0] += 1\n",
        "      if predictions[i] == 0 and labels[i] == 1:\n",
        "        matrix1[0,1] += 1\n",
        "      if predictions[i] == 1 and labels[i] == 0:\n",
        "        matrix1[1,0] += 1\n",
        "      if predictions[i] == 0 and labels[i] == 0:\n",
        "        matrix1[1,1] += 1\n",
        "  \n",
        "  precision = matrix1[0,0] / (matrix1[0,0] + matrix1[0,1])\n",
        "  recall = matrix1[0,0] / (matrix1[0,0] + matrix1[1,0])\n",
        "  f1= 2 * (precision * recall) / (precision + recall)\n",
        "  classification_report.loc['class 1'] = [precision, recall, f1]\n",
        "\n",
        "  print(classification_report)\n",
        "\n",
        "  #################### EDIT ABOVE THIS LINE #########################\n",
        "\n",
        "# Calculate evaluation score based on the metric of your choice\n",
        "# for the classifier trained in Task 2 using tf-idf features.\n",
        "evaluate(test_labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22OelF89a27J"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 4: Feature Engineering (II) - Other features (10 Marks)\n",
        "\n",
        "Describe features other than those defined in Task 2 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EBS0F877UyC"
      },
      "source": [
        "Edit this cell to write your answer below the line in no more than 500 words.\n",
        "\n",
        "---\n",
        "There are two things which I feel can change the accuracy and evaluation of the score.\n",
        "\n",
        "1. **Bag of n-grams:** A bag of n-grams model provides not only with the words in the sentence, but also the n-grams of the sentences. There can be a different meaning to 2 or 3 words when we use together e.g., 'the dog barks' and 'the dog barked' has a difference of grammatical tense and changes the way humans interpret the sentence, so in similar way we can vectorize n-grams to get good TF-IDF values to process our model well. In our case we have used *trigrams* to fit our model. This doesn't require any additional preprocessing steps than what we have used.\n",
        "\n",
        "2. **Multinomial Naive Bayes:** Multinomial Naive Bayes is a probabilistic learning algorithm used to classify labels by calculating probabilities and then comparing them to a threshold which is usually set by checking our for the line which separates the given classes in the mathematical space. Multinomial Naïve Bayes consider a feature vector where a given term represents the number of times it appears or very often i.e. frequency. We use the MultinomialNB() class from the *sklean.naive_bayes* package to use them. There is a hyperparameter called *alpha* which is used for smoothening of the algorithm so that it is not as steep when classifying. We are using 0.03 as the alpha value (found through just trying different values, getting better score.)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfkzM3DRce14"
      },
      "source": [
        "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
        "\n",
        "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 3. You **must not** use the test set for training.\n",
        "\n",
        "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u9mRku0va8kK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        precision    recall  f1-score\n",
            "class 0  0.925024  0.831146  0.875576\n",
            "class 1   0.42042  0.645161  0.509091\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8014705882352942"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create your features.\n",
        "# ... your code goes here\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
        "tfidf = TfidfTransformer()\n",
        "\n",
        "v_train2 = vectorizer2.fit_transform(train_texts)\n",
        "v_test2 = vectorizer2.transform(test_texts)\n",
        "\n",
        "tf_train2 = tfidf.fit_transform(v_train2)\n",
        "tf_test2 = tfidf.transform(v_test2)\n",
        "\n",
        "# Train a Naïve Bayes classifier using the features you defined.\n",
        "# ... your code goes here\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "NB_classifier2 = MultinomialNB(alpha= 0.03)\n",
        "NB_classifier2.fit(tf_train2.toarray(), train_labels)\n",
        "\n",
        "\n",
        "# Evaluate on the test set.\n",
        "# ... your code goes here\n",
        "predictions2 = NB_classifier2.predict(tf_test2.toarray())\n",
        "evaluate(test_labels, predictions2)\n",
        "accuracy(test_labels, predictions2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyDD1zFQdwCf"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 5: Kaggle Competition (10 marks)\n",
        "\n",
        "Head over to https://www.kaggle.com/t/1f90b74da0b7484da9647638e22d1068  \n",
        "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. Please note that the evaluation metric for the competition is the f-score.\n",
        "\n",
        "Read competition page for more details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9NZrBayoN4A",
        "outputId": "d2c338a4-f20f-429e-9c69-a4a7850de428"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JaC6B824Fe0H"
      },
      "outputs": [],
      "source": [
        "# Preparing submission for Kaggle\n",
        "StudentID = \"22222806_Rana\" # Please add your student id and lastname\n",
        "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
        "utest_texts = test_unseen[\"text\"].to_list()\n",
        "test_vect = vectorizer2.transform(utest_texts)\n",
        "test_tfidf = tfidf.transform(test_vect)\n",
        "test_pred = NB_classifier2.predict(test_tfidf.toarray())\n",
        "\n",
        "\n",
        "# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\n",
        "# Expected is a list of prediction made by your classifier\n",
        "\n",
        "sub = {\"Id\": [f\"test_{i}\" for i in range(len(test_unseen))],\n",
        "       \"Expected\": test_pred}\n",
        "sub_df = pd.DataFrame(sub)\n",
        "# The code below will generate a StudentID.csv on your drive on the left hand side in the explorer\n",
        "# Please upload the file as a submission on the competition page\n",
        "# You can index your submission StudentID_Lastname_index.csv, where index is your number of submission\n",
        "sub_df.to_csv(f\"{StudentID}.csv\", sep=\",\", header=1, index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6brptmkqY9C"
      },
      "source": [
        "Mention the approach that you have chosen briefly, and what is the mean average f-score that you have achieved? Did it improve above the chosen baseline model (0.36823)? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZQumdT-9yet"
      },
      "source": [
        "Edit this cell to write your answer below the line in no more than 500 words.\n",
        "\n",
        "---\n",
        "We have used the MultinomialNB instead of GaussianNB to fit our test cases as we did a trigram fit of our training case to train the model. We achieved a mean average f-score on 0.79117 on Kaggle. I feel that this method was crucial as we wanted to find whether a statement was a suggestion or not. In order to do so, we have to weight not only the words, but the words around it and I felt that having trigrams would be much better to weight and put in the model. It definitely improved when we run the training model and evaluated them. So that was the experimental motivation which led me to believe that this model may work well to surpass the baseline score. And it worked well as the models are better given the n-gram approach when finding something like a suggestion mining model where the position of the texts and their trigram weight matters more in the general context.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "895965ff89e31dbb8817b13ea40d8d3de3483b91af9cca7b5616b48a8b2876f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
